# ğŸ§  Sign Language Gesture Recognition Using CNN & MediaPipe

This project is a **real-time sign language recognition system** that translates American Sign Language (ASL) gestures into text and speech using computer vision and deep learning. It leverages **MediaPipe** for hand tracking and a custom-trained **Convolutional Neural Network (CNN)** for gesture classification.

> ğŸ”— Ideal for: Accessibility applications, speech/hearing impairment support, and human-computer interaction.

---

## ğŸ“Œ Key Features

- ğŸ” **Real-time Webcam-based Recognition**  
  No need for gloves or sensors â€” just a camera.

- ğŸ§  **CNN Model Trained on ASL Dataset**  
  Trained using the [ASL Alphabet Dataset](https://www.kaggle.com/datasets/grassknoted/asl-alphabet).

- ğŸ¯ **Accurate Gesture Detection with MediaPipe**  
  Uses MediaPipe Hands for precise hand landmark extraction.

- ğŸ’¬ **Dual Mode Output (Text & Speech)**  
  Displays recognized gestures and optionally speaks them aloud using `pyttsx3`.

- ğŸ’» **Simple GUI Interface**  
  Built with Pythonâ€™s Tkinter for user-friendly operation.

---

## ğŸ”§ Tech Stack

| Tool / Library       | Purpose                                |
|----------------------|----------------------------------------|
| Python 3.9           | Core programming language              |
| OpenCV               | Webcam and image processing            |
| MediaPipe            | Hand landmark detection                |
| TensorFlow/Keras     | Model training and inference           |
| NumPy                | Data manipulation                      |
| pyttsx3              | Text-to-Speech (TTS) engine            |
| Tkinter              | GUI development                        |

---

## ğŸ“ Folder Structure

```
SignLanguageInterpreter/
â”‚
â”œâ”€â”€ gesture_interpreter.py      # Main interpreter script
â”œâ”€â”€ train_model.py              # CNN model training
â”œâ”€â”€ reduce_dataset.py           # Optional dataset balancing
â”œâ”€â”€ gesture_cnn_model.h5        # Trained CNN model
â”œâ”€â”€ dataset/                    # ASL alphabet images
â”œâ”€â”€ label_encoder.pkl           # Label mapping for gestures
â”œâ”€â”€ models/                     # Folder for saving models
â”œâ”€â”€ README.md                   # This file
â””â”€â”€ venv/                       # Python virtual environment
```

---

## âš™ï¸ How to Run

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/SignLanguageInterpreter.git
   cd SignLanguageInterpreter
   ```

2. Install requirements:
   ```bash
   pip install -r requirements.txt
   ```

3. Run the interpreter:
   ```bash
   python gesture_interpreter.py
   ```

---

## ğŸ“Œ Dataset Details

- **Source**: [Kaggle - ASL Alphabet](https://www.kaggle.com/datasets/grassknoted/asl-alphabet)
- **Classes**: A-Z letters, space, delete, nothing
- **Image Size**: 64x64 grayscale, preprocessed from cropped hand regions

---

## ğŸ“ Use Cases

- Assisting the **speech or hearing impaired**
- Integration into **education systems**
- Prototype for **gesture-based interfaces**

---

## ğŸ™Œ Contributors

- Ambika (Final Year B.Tech Project)
- Guidance: [Add Faculty/Guide Name if needed]

---

## ğŸ“ƒ License

This project is open-source and available under the **MIT License**.
